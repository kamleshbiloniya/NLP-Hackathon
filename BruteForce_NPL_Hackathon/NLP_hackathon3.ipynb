{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_hackathon3.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "qqSwHw6x9Tsl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import sys\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "from sys import path\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import matplotlib.pyplot as plt\n",
        "from string import punctuation, digits\n",
        "from IPython.core.display import display, HTML\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-0FhC2Pf9XjN",
        "colab_type": "code",
        "outputId": "94f90c9e-1bfd-45e0-8639-866122be4069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S_KR3Pu59z6r",
        "colab_type": "code",
        "outputId": "a5cd79cf-058f-4353-a37f-0e26350baa24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/gdrive/My Drive/\")\n",
        "os.getcwd()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/gdrive/My Drive'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "pzW3io3N-2rT",
        "colab_type": "code",
        "outputId": "8adacd90-88e4-4b67-c307-6e25037cb8f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "! pip install googletrans"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googletrans in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from googletrans) (2.18.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (2018.11.29)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bx4W_5T4-9f8",
        "colab_type": "code",
        "outputId": "dab65323-c006-4ab1-cc70-53954e495989",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HnmlDL0dvKWG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a9d9165b-1af8-49ef-9d37-edca6e46c26b"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "0Q4uL9vQvUbu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "41997784-064c-4dc5-dbbc-1f7962536023"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "y0gUbSGpa7Pt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# google translation\n",
        "from googletrans import Translator\n",
        "# from google.cloud import translate\n",
        "import emoji,json,copy\n",
        "def translate_to_eng(data):\n",
        "    translator = Translator()\n",
        "    for i in range(1,data_size):\n",
        "            inpt = emoji.demojize(data[i])\n",
        "            print(lang[i],str(inpt))\n",
        "            translation = translator.translate(str(inpt),dest='en')\n",
        "            print(translation.text)\n",
        "            data[i] = str(translation.text)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g803Fa0lbBe1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# training data1\n",
        "fp = open(\"./preprocessed-twitter-tweets/processedPositive.txt\")\n",
        "data = fp.read()\n",
        "data_pos = data.split(',')\n",
        "label_pos = np.array(np.ones(len(data_pos)))\n",
        "fp = open(\"./preprocessed-twitter-tweets/processedNegative.txt\")\n",
        "data = fp.read()\n",
        "data_neg = data.split(',')\n",
        "label_neg = np.array(2 * np.ones(len(data_neg)))\n",
        "fp = open(\"./preprocessed-twitter-tweets/processedNeutral.txt\")\n",
        "data = fp.read()\n",
        "data_neu = data.split(',')\n",
        "label_neu = np.array(np.zeros(len(data_neu)))\n",
        "\n",
        "for i in range(0,len(data_pos)):\n",
        "    data_pos[i] = data_pos[i] +\" $$ 1\" \n",
        "for i in range(0,len(data_neg)):\n",
        "    data_neg[i] = data_neg[i] +\" $$ 2\" \n",
        "for i in range(0,len(data_neu)):\n",
        "    data_neu[i] = data_neu[i] +\" $$ 0\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AmVvQqOzbJ_2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#training data 2\n",
        "fp = open(\"gold-post-eng.txt\")\n",
        "gold_data = fp.read().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OGMkfeNmb20o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "91ab2817-a3b3-4805-ae58-a09e873e19de"
      },
      "cell_type": "code",
      "source": [
        "data_all = data_pos + data_neg + data_neu + gold_data\n",
        "print(len(data_all))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PFuDWgSgb6bW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed2bbcd8-7ab5-4e8c-98dc-b452a4f79554"
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(data_all)\n",
        "data = []\n",
        "labels = []\n",
        "count = 0\n",
        "for i in range(0,len(data_all)):\n",
        "    if len(data_all[i].split('$$')) == 2:\n",
        "        try:\n",
        "            labels.append(int(data_all[i].split('$$')[1]))\n",
        "            data.append(data_all[i].split('$$')[0])\n",
        "        except:\n",
        "            count +=1\n",
        "\n",
        "print(len(data),len(labels))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11665 11665\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3SdFu8ZacXvq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "def preprocessing(eng_data):\n",
        "    for i in range(0,len(eng_data)):\n",
        "        eng_data[i] = eng_data[i].lower()\n",
        "        eng_data[i] = re.sub('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '',eng_data[i])\n",
        "        eng_data[i] = re.sub('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '',eng_data[i])\n",
        "        eng_data[i] = re.sub('[^\\w\\s]','',eng_data[i])\n",
        "        eng_data[i] = re.sub('\\d', '',eng_data[i])\n",
        "        word_tokens = word_tokenize(eng_data[i])\n",
        "        filtered_sentence = [w for w in word_tokens if not w in stopwords.words('english')]\n",
        "        sent = \"\"\n",
        "        i = 0\n",
        "        for w in filtered_sentence:\n",
        "            if i ==0:\n",
        "                sent += w\n",
        "                i+=1\n",
        "            else:\n",
        "                sent += \" \"+w\n",
        "        eng_data[i] = sent\n",
        "    return eng_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PkElcbptc6gz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73460f8f-019b-4edc-ddf6-d22a1bfe7d11"
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data,labels, test_size=0.10, random_state=0)\n",
        "classes = np.array([0,1,2])\n",
        "\n",
        "print(len(X_train),len(X_test),len(y_train),len(y_test))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10498 1167 10498 1167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QLAL6ksIdHLD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iHhek0H_dLvn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "b017b1f8-312b-4841-9cb3-5b5daf9012e3"
      },
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(analyzer='word',ngram_range=(1,2), max_features=50000,max_df=0.5,use_idf=True, norm='l2') \n",
        "counts = vectorizer.fit_transform(X_train)\n",
        "vocab = vectorizer.vocabulary_\n",
        "classifier = SGDClassifier(alpha=1e-05,max_iter=50,penalty='elasticnet')\n",
        "targets = y_train\n",
        "classifier = classifier.fit(counts, targets)\n",
        "example_counts = vectorizer.transform(X_test)\n",
        "predictions = classifier.predict(example_counts)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "7FpjcZdPdOp9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NHNH7baodR98",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bc01688d-1282-4fa6-8979-bb9e7ccb9c53"
      },
      "cell_type": "code",
      "source": [
        "acc = accuracy_score(y_test, predictions, normalize=True)\n",
        "hit = precision_score(y_test, predictions, average=None,labels=classes)\n",
        "capture = recall_score(y_test, predictions, average=None,labels=classes)\n",
        "print('Model Accuracy:%.2f'%acc)\n",
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Accuracy:0.76\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.81      0.79       582\n",
            "           1       0.77      0.79      0.78       329\n",
            "           2       0.68      0.61      0.64       256\n",
            "\n",
            "   micro avg       0.76      0.76      0.76      1167\n",
            "   macro avg       0.74      0.73      0.74      1167\n",
            "weighted avg       0.75      0.76      0.75      1167\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AWQBMxuJdVeZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model saving\n",
        "import pickle\n",
        "filename = 'Model_nlp_hackathon.sav'\n",
        "pickle.dump(classifier, open(filename, 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bJtLmggBePSN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#testing Model\n",
        "# loading test data \n",
        "import xlrd\n",
        "wb = xlrd.open_workbook('./Languages_Sentiment_labels2.xlsx')\n",
        "sheet = wb.sheet_by_index(0)\n",
        "data_test = []\n",
        "labels_code_test = []\n",
        "labels_test =[]\n",
        "lang_test = []\n",
        "data_size = 413\n",
        "for i in range(1,data_size):\n",
        "    labels_test.append(sheet.cell_value(i,0))\n",
        "    lang_test.append(sheet.cell_value(i,1))\n",
        "    data_test.append(sheet.cell_value(i,2)) \n",
        "\n",
        "n=0\n",
        "p=0\n",
        "neg=0\n",
        "data_size = len(data_test)\n",
        "for i in range(0,len(data_test)):\n",
        "    if labels_test[i] == \"neutral\":\n",
        "        labels_code_test.append(0)\n",
        "        n+=1\n",
        "    elif labels_test[i] == \"positive\":\n",
        "        labels_code_test.append(1)\n",
        "        p+=1\n",
        "    else:\n",
        "        labels_code_test.append(2)\n",
        "        neg+=1\n",
        "        \n",
        "data_test = translate_to_eng(data_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Dfye2LLuy2b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a0b0ee70-6e0d-49ed-c1a6-5d790184b415"
      },
      "cell_type": "code",
      "source": [
        "example_counts = vectorizer.transform(preprocessing(data_test))\n",
        "predictions = classifier.predict(example_counts)\n",
        "acc = accuracy_score(labels_code_test, predictions, normalize=True)\n",
        "hit = precision_score(labels_code_test, predictions, average=None,labels=classes)\n",
        "capture = recall_score(labels_code_test, predictions, average=None,labels=classes)\n",
        "print('Model Accuracy:%.2f'%acc)\n",
        "print(classification_report(labels_code_test, predictions))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Accuracy:0.44\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.53      0.54       176\n",
            "           1       0.37      0.61      0.46       132\n",
            "           2       0.43      0.09      0.14       104\n",
            "\n",
            "   micro avg       0.44      0.44      0.44       412\n",
            "   macro avg       0.45      0.41      0.38       412\n",
            "weighted avg       0.46      0.44      0.41       412\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C7y69RcMvCGz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7038
        },
        "outputId": "fe5408ff-a373-44e9-ffb8-512ac1d5c441"
      },
      "cell_type": "code",
      "source": [
        "print(\"Language\",\"text\",\"True_sentiment\",\"predicted_sentiment\")\n",
        "labels_t = []\n",
        "labels_p = []\n",
        "for i in range(0,len(labels_code_test)):\n",
        "  if labels_code_test[i]==0:\n",
        "    labels_t.append(\"neutral\")\n",
        "  elif labels_code_test[i] == 1:\n",
        "    labels_t.append(\"positive\")\n",
        "  else:\n",
        "    labels_t.append(\"negative\")\n",
        "for i in range(0,len(predictions)):\n",
        "  if predictions[i]==0:\n",
        "    labels_p.append(\"neutral\")\n",
        "  elif predictions[i] == 1:\n",
        "    labels_p.append(\"positive\")\n",
        "  else:\n",
        "    labels_p.append(\"negative\")    \n",
        "for i in range(0,len(labels_code_test)):\n",
        "  print(lang_test[i],labels_t[i], labels_p[i])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Language text True_sentiment predicted_sentiment\n",
            "bn positive positive\n",
            "bn neutral positive\n",
            "bn negative neutral\n",
            "bn neutral neutral\n",
            "bn negative neutral\n",
            "bn neutral neutral\n",
            "bn neutral neutral\n",
            "bn neutral neutral\n",
            "bn positive neutral\n",
            "bn neutral neutral\n",
            "bn negative neutral\n",
            "bn negative neutral\n",
            "bn negative neutral\n",
            "bn neutral neutral\n",
            "bn positive positive\n",
            "bn positive neutral\n",
            "bn neutral neutral\n",
            "bn negative neutral\n",
            "bn negative neutral\n",
            "bn positive neutral\n",
            "bn neutral neutral\n",
            "bn positive positive\n",
            "bn negative neutral\n",
            "bn neutral neutral\n",
            "bn positive positive\n",
            "bn neutral neutral\n",
            "bn positive neutral\n",
            "bn positive neutral\n",
            "bn positive positive\n",
            "bn positive neutral\n",
            "bn positive neutral\n",
            "bn negative neutral\n",
            "bn neutral neutral\n",
            "bn positive neutral\n",
            "bn positive neutral\n",
            "bn negative neutral\n",
            "bn positive neutral\n",
            "bn neutral neutral\n",
            "bn positive neutral\n",
            "bn neutral neutral\n",
            "bn neutral neutral\n",
            "bn positive neutral\n",
            "bn positive positive\n",
            "bn positive neutral\n",
            "bn positive neutral\n",
            "bn neutral neutral\n",
            "bn neutral positive\n",
            "bn neutral neutral\n",
            "bn neutral neutral\n",
            "bn neutral positive\n",
            "bn negative neutral\n",
            "bn negative neutral\n",
            "bn positive neutral\n",
            "bn neutral neutral\n",
            "bn neutral neutral\n",
            "bn positive neutral\n",
            "bn neutral neutral\n",
            "bn neutral neutral\n",
            "bn neutral neutral\n",
            "en neutral neutral\n",
            "en negative neutral\n",
            "en negative positive\n",
            "en positive positive\n",
            "en negative positive\n",
            "en positive positive\n",
            "en positive positive\n",
            "en negative negative\n",
            "en neutral neutral\n",
            "en positive negative\n",
            "en positive positive\n",
            "en positive negative\n",
            "en neutral neutral\n",
            "en positive positive\n",
            "en negative negative\n",
            "en neutral neutral\n",
            "en neutral positive\n",
            "en neutral neutral\n",
            "en neutral neutral\n",
            "en neutral neutral\n",
            "en positive positive\n",
            "en neutral negative\n",
            "en negative neutral\n",
            "en negative positive\n",
            "en neutral neutral\n",
            "en positive neutral\n",
            "en positive negative\n",
            "en neutral neutral\n",
            "en neutral negative\n",
            "en neutral neutral\n",
            "en positive neutral\n",
            "en positive neutral\n",
            "en neutral neutral\n",
            "en neutral neutral\n",
            "en neutral positive\n",
            "en neutral neutral\n",
            "en positive positive\n",
            "en neutral positive\n",
            "en negative neutral\n",
            "en positive negative\n",
            "en negative neutral\n",
            "en positive neutral\n",
            "en negative negative\n",
            "en positive positive\n",
            "en positive positive\n",
            "en positive neutral\n",
            "en neutral negative\n",
            "en positive positive\n",
            "en negative positive\n",
            "en negative negative\n",
            "en positive neutral\n",
            "en positive positive\n",
            "en positive positive\n",
            "en positive neutral\n",
            "en neutral neutral\n",
            "en positive neutral\n",
            "en negative negative\n",
            "en positive positive\n",
            "en positive positive\n",
            "en positive neutral\n",
            "en negative positive\n",
            "en negative negative\n",
            "en negative negative\n",
            "en neutral neutral\n",
            "en neutral negative\n",
            "en positive positive\n",
            "en neutral neutral\n",
            "en negative neutral\n",
            "en positive negative\n",
            "en neutral positive\n",
            "en neutral positive\n",
            "en positive positive\n",
            "en neutral positive\n",
            "en negative negative\n",
            "en negative neutral\n",
            "en negative neutral\n",
            "en neutral neutral\n",
            "en neutral neutral\n",
            "en neutral positive\n",
            "en positive negative\n",
            "en positive neutral\n",
            "en positive positive\n",
            "en neutral neutral\n",
            "en neutral neutral\n",
            "en neutral neutral\n",
            "en neutral neutral\n",
            "en positive negative\n",
            "en neutral positive\n",
            "en positive positive\n",
            "en neutral neutral\n",
            "en neutral neutral\n",
            "en neutral neutral\n",
            "en positive neutral\n",
            "en negative negative\n",
            "en positive neutral\n",
            "en positive neutral\n",
            "en neutral negative\n",
            "en positive positive\n",
            "en neutral neutral\n",
            "en neutral positive\n",
            "en negative positive\n",
            "hi positive positive\n",
            "hi positive positive\n",
            "hi neutral positive\n",
            "hi positive positive\n",
            "hi neutral positive\n",
            "hi neutral neutral\n",
            "hi positive positive\n",
            "hi negative positive\n",
            "hi neutral neutral\n",
            "hi neutral neutral\n",
            "hi negative neutral\n",
            "hi neutral neutral\n",
            "hi neutral neutral\n",
            "hi neutral neutral\n",
            "hi positive neutral\n",
            "hi negative positive\n",
            "hi negative positive\n",
            "hi neutral positive\n",
            "hi neutral positive\n",
            "hi neutral positive\n",
            "hi neutral positive\n",
            "hi neutral positive\n",
            "hi negative positive\n",
            "hi neutral positive\n",
            "hi positive positive\n",
            "hi positive neutral\n",
            "hi neutral positive\n",
            "hi neutral positive\n",
            "hi neutral neutral\n",
            "hi positive neutral\n",
            "hi neutral neutral\n",
            "hi neutral neutral\n",
            "hi negative neutral\n",
            "hi negative positive\n",
            "hi negative neutral\n",
            "hi negative positive\n",
            "hi neutral positive\n",
            "hi negative neutral\n",
            "hi neutral positive\n",
            "hi positive neutral\n",
            "hi neutral neutral\n",
            "hi neutral neutral\n",
            "hi neutral neutral\n",
            "hi negative neutral\n",
            "hi negative neutral\n",
            "hi neutral positive\n",
            "hi negative neutral\n",
            "hi neutral positive\n",
            "hi neutral neutral\n",
            "mr neutral neutral\n",
            "mr positive neutral\n",
            "mr positive neutral\n",
            "mr positive positive\n",
            "mr neutral positive\n",
            "mr positive positive\n",
            "mr neutral positive\n",
            "mr neutral positive\n",
            "mr positive positive\n",
            "mr positive positive\n",
            "mr neutral positive\n",
            "mr negative neutral\n",
            "mr neutral positive\n",
            "mr negative positive\n",
            "mr neutral positive\n",
            "mr positive positive\n",
            "mr neutral neutral\n",
            "mr positive positive\n",
            "mr positive positive\n",
            "mr neutral neutral\n",
            "mr positive neutral\n",
            "mr positive positive\n",
            "mr neutral positive\n",
            "mr neutral positive\n",
            "mr negative neutral\n",
            "mr positive positive\n",
            "mr neutral positive\n",
            "mr neutral neutral\n",
            "mr positive neutral\n",
            "mr positive positive\n",
            "mr neutral positive\n",
            "mr neutral neutral\n",
            "mr negative positive\n",
            "mr neutral neutral\n",
            "mr neutral positive\n",
            "mr positive positive\n",
            "mr neutral neutral\n",
            "mr positive positive\n",
            "mr neutral neutral\n",
            "mr positive positive\n",
            "mr neutral positive\n",
            "mr positive positive\n",
            "mr neutral neutral\n",
            "mr neutral neutral\n",
            "mr neutral positive\n",
            "mr positive positive\n",
            "mr neutral positive\n",
            "mr neutral positive\n",
            "ta neutral positive\n",
            "ta neutral positive\n",
            "ta neutral neutral\n",
            "ta positive positive\n",
            "ta neutral neutral\n",
            "ta neutral positive\n",
            "ta neutral neutral\n",
            "ta neutral positive\n",
            "ta positive positive\n",
            "ta positive neutral\n",
            "ta negative neutral\n",
            "ta neutral neutral\n",
            "ta neutral positive\n",
            "ta positive positive\n",
            "ta neutral positive\n",
            "ta neutral positive\n",
            "ta neutral positive\n",
            "ta positive positive\n",
            "ta negative positive\n",
            "ta negative positive\n",
            "ta neutral positive\n",
            "ta positive neutral\n",
            "ta neutral positive\n",
            "ta neutral positive\n",
            "ta positive neutral\n",
            "ta neutral positive\n",
            "ta positive positive\n",
            "ta negative positive\n",
            "ta neutral positive\n",
            "ta positive neutral\n",
            "ta positive positive\n",
            "ta neutral neutral\n",
            "ta neutral neutral\n",
            "ta positive neutral\n",
            "ta neutral positive\n",
            "ta neutral positive\n",
            "ta neutral positive\n",
            "ta neutral positive\n",
            "ta negative neutral\n",
            "ta positive positive\n",
            "ta neutral positive\n",
            "ta positive positive\n",
            "ta positive positive\n",
            "ta neutral positive\n",
            "ta neutral neutral\n",
            "ta negative positive\n",
            "ta neutral positive\n",
            "ta neutral positive\n",
            "ta negative positive\n",
            "ta neutral neutral\n",
            "ta neutral neutral\n",
            "ta neutral positive\n",
            "te positive positive\n",
            "te neutral neutral\n",
            "te positive positive\n",
            "te negative positive\n",
            "te negative positive\n",
            "te positive positive\n",
            "te negative positive\n",
            "te negative neutral\n",
            "te negative positive\n",
            "te positive positive\n",
            "te negative positive\n",
            "te positive positive\n",
            "te neutral neutral\n",
            "te negative positive\n",
            "te negative positive\n",
            "te negative positive\n",
            "te positive positive\n",
            "te negative positive\n",
            "te neutral positive\n",
            "te neutral positive\n",
            "te negative positive\n",
            "te positive positive\n",
            "te positive positive\n",
            "te negative positive\n",
            "te neutral positive\n",
            "te negative positive\n",
            "te negative positive\n",
            "te negative positive\n",
            "te negative positive\n",
            "te neutral neutral\n",
            "te positive positive\n",
            "te neutral positive\n",
            "te negative positive\n",
            "te negative positive\n",
            "te negative positive\n",
            "te neutral neutral\n",
            "te negative positive\n",
            "te neutral positive\n",
            "te neutral positive\n",
            "te negative positive\n",
            "te positive positive\n",
            "te positive positive\n",
            "te positive neutral\n",
            "te negative positive\n",
            "te positive positive\n",
            "te positive positive\n",
            "te positive neutral\n",
            "te negative positive\n",
            "te positive positive\n",
            "te negative positive\n",
            "te neutral positive\n",
            "te negative positive\n",
            "te negative neutral\n",
            "te neutral positive\n",
            "te neutral neutral\n",
            "te positive positive\n",
            "te negative positive\n",
            "te negative positive\n",
            "te neutral positive\n",
            "te negative positive\n",
            "te positive positive\n",
            "te neutral positive\n",
            "te positive positive\n",
            "te negative positive\n",
            "te neutral positive\n",
            "te negative positive\n",
            "te negative positive\n",
            "te neutral positive\n",
            "te negative positive\n",
            "te neutral neutral\n",
            "te positive positive\n",
            "te negative neutral\n",
            "te negative positive\n",
            "te negative positive\n",
            "te neutral neutral\n",
            "te neutral positive\n",
            "te neutral neutral\n",
            "te negative positive\n",
            "te positive positive\n",
            "te positive positive\n",
            "te negative positive\n",
            "te positive positive\n",
            "te negative positive\n",
            "te positive positive\n",
            "te neutral neutral\n",
            "te positive positive\n",
            "te neutral neutral\n",
            "te neutral neutral\n",
            "te positive positive\n",
            "te neutral neutral\n",
            "te negative positive\n",
            "te neutral positive\n",
            "te negative positive\n",
            "te positive positive\n",
            "te positive positive\n",
            "te negative positive\n",
            "te neutral neutral\n",
            "te neutral positive\n",
            "te negative positive\n",
            "te positive positive\n",
            "te negative positive\n",
            "te negative neutral\n",
            "te negative positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LxhTzaIsV8eT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}